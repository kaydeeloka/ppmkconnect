{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6358196,"sourceType":"datasetVersion","datasetId":3579787},{"sourceId":8575334,"sourceType":"datasetVersion","datasetId":5127760},{"sourceId":8171572,"sourceType":"datasetVersion","datasetId":4836275}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchaudio librosa\n\nimport os\nimport librosa\nimport numpy as np\nimport torch\nimport torchaudio\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob\nfrom tqdm import tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:41:48.802533Z","iopub.execute_input":"2025-09-29T04:41:48.802807Z","iopub.status.idle":"2025-09-29T04:41:52.263289Z","shell.execute_reply.started":"2025-09-29T04:41:48.802789Z","shell.execute_reply":"2025-09-29T04:41:52.262213Z"},"_kg_hide-output":false},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\nRequirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\nRequirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\nRequirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.2)\nRequirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.2.2)\nRequirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.0)\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\nRequirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\nRequirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\nRequirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\nRequirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\nRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.22.3->librosa) (2.4.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.3->librosa) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.22.3->librosa) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.3->librosa) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.22.3->librosa) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.22.3->librosa) (2024.2.0)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"class FusionAudioDataset(Dataset):\n    def __init__(self, real_files, fake_files):\n        self.samples = [(f, 0) for f in real_files] + [(f, 1) for f in fake_files]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        path, label = self.samples[idx]\n        # Raw waveform\n        waveform, sr = torchaudio.load(path)\n        waveform = waveform[:, :16000]  # Truncate or pad to 1 sec\n        if waveform.shape[1] < 16000:\n            pad = 16000 - waveform.shape[1]\n            waveform = torch.nn.functional.pad(waveform, (0, pad))\n\n        # Mel spectrogram\n        mel = audio_to_mel(path)\n        mel = torch.tensor(mel).unsqueeze(0)\n\n        return waveform.float(), mel.float(), torch.tensor(label).long()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:41:52.265025Z","iopub.execute_input":"2025-09-29T04:41:52.265374Z","iopub.status.idle":"2025-09-29T04:41:52.271910Z","shell.execute_reply.started":"2025-09-29T04:41:52.265338Z","shell.execute_reply":"2025-09-29T04:41:52.271289Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchaudio\nimport librosa\nimport numpy as np\n\nclass DeepfakeAudioDataset(Dataset):\n    def __init__(self, root_dir, sample_rate=16000, duration=1.0):\n        self.root_dir = root_dir\n        self.sample_rate = sample_rate\n        self.duration = duration\n        self.audio_paths = []\n        self.labels = []\n\n        # change the label based on the file name.\n        for label_str, label in [('real', 0), ('fake', 1)]:\n            folder = os.path.join(root_dir, label_str)\n            for file in os.listdir(folder):\n                #follow what the file format\n                if file.endswith(\".wav\"):\n                    self.audio_paths.append(os.path.join(folder, file))\n                    self.labels.append(label)\n\n    def __len__(self):\n        return len(self.audio_paths)\n\n    def __getitem__(self, idx):\n        path = self.audio_paths[idx]\n        label = self.labels[idx]\n\n        waveform, sr = torchaudio.load(path)\n        waveform = waveform.mean(dim=0)  # mono\n        waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n\n        # Pad/crop to fixed duration\n        max_len = int(self.sample_rate * self.duration)\n        if waveform.shape[0] < max_len:\n            waveform = torch.nn.functional.pad(waveform, (0, max_len - waveform.shape[0]))\n        else:\n            waveform = waveform[:max_len]\n\n        # Mel spectrogram\n        mel = librosa.feature.melspectrogram(y=waveform.numpy(), sr=self.sample_rate, n_mels=128)\n        mel_db = librosa.power_to_db(mel, ref=np.max)\n        mel_tensor = torch.tensor(mel_db).unsqueeze(0).float()  # (1, 128, T)\n\n        return waveform.unsqueeze(0).float(), mel_tensor, torch.tensor(label).long()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:42:08.021007Z","iopub.execute_input":"2025-09-29T04:42:08.021327Z","iopub.status.idle":"2025-09-29T04:42:08.030789Z","shell.execute_reply.started":"2025-09-29T04:42:08.021304Z","shell.execute_reply":"2025-09-29T04:42:08.029841Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"class SpecRNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((32, 32))\n        )\n        self.fc = nn.Linear(64 * 32 * 32, 128)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:41:52.295328Z","iopub.execute_input":"2025-09-29T04:41:52.295813Z","iopub.status.idle":"2025-09-29T04:41:52.311478Z","shell.execute_reply.started":"2025-09-29T04:41:52.295796Z","shell.execute_reply":"2025-09-29T04:41:52.310843Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class RawGATST(nn.Module):\n    def __init__(self):\n        super(RawGATST, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(1, 64, kernel_size=5, stride=2, padding=2),\n            nn.BatchNorm1d(64), nn.ReLU(),\n            nn.Conv1d(64, 128, kernel_size=5, stride=2, padding=2),\n            nn.BatchNorm1d(128), nn.ReLU()\n        )\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(128, 128)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.pool(x).squeeze(-1)\n        x = self.fc(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:41:52.312188Z","iopub.execute_input":"2025-09-29T04:41:52.312431Z","iopub.status.idle":"2025-09-29T04:41:52.327753Z","shell.execute_reply.started":"2025-09-29T04:41:52.312415Z","shell.execute_reply":"2025-09-29T04:41:52.327028Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class FusionNet(nn.Module):\n    def __init__(self):\n        super(FusionNet, self).__init__()\n        self.spec_model = SpecRNet()\n        self.raw_model = RawGATST()\n        self.classifier = nn.Sequential(\n            nn.Linear(128 + 128, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 2)\n        )\n\n    def forward(self, raw_wave, mel_spec):\n        raw_feat = self.raw_model(raw_wave)\n        spec_feat = self.spec_model(mel_spec)\n        fused = torch.cat((raw_feat, spec_feat), dim=1)\n        return self.classifier(fused)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:41:52.328586Z","iopub.execute_input":"2025-09-29T04:41:52.329095Z","iopub.status.idle":"2025-09-29T04:41:52.342518Z","shell.execute_reply.started":"2025-09-29T04:41:52.329066Z","shell.execute_reply":"2025-09-29T04:41:52.341681Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from torch.utils.data import Subset\nfrom sklearn.model_selection import train_test_split\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Get dataset\ndataset = DeepfakeAudioDataset(\"/kaggle/input/in-the-wild-audio-deepfake/release_in_the_wild\")\n\n# Create index list\nindices = list(range(len(dataset)))\n\n# Split into train + temp (val+test)\ntrain_idx, temp_idx = train_test_split(indices, test_size=0.4, random_state=42)  # 60% train, 40% temp\n\n# Split temp into validation + test (each 20%)\nval_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)  # 20% val, 20% test\n\n# Wrap subsets\ntrain_set = Subset(dataset, train_idx)\nval_set   = Subset(dataset, val_idx)\ntest_set  = Subset(dataset, test_idx)\n\n# DataLoaders\ntrain_loader = DataLoader(train_set, batch_size=8, shuffle=True, num_workers=2)\nval_loader   = DataLoader(val_set, batch_size=8, shuffle=True, num_workers=2)\ntest_loader  = DataLoader(test_set, batch_size=8, shuffle=True, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:42:17.566659Z","iopub.execute_input":"2025-09-29T04:42:17.566956Z","iopub.status.idle":"2025-09-29T04:42:17.575758Z","shell.execute_reply.started":"2025-09-29T04:42:17.566935Z","shell.execute_reply":"2025-09-29T04:42:17.575119Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"model = FusionNet().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nloss_fn = nn.CrossEntropyLoss()\n\n# Training loop\nfor epoch in range(20):\n    model.train()\n    total_loss = 0\n    correct = 0\n    for raw, mel, labels in train_loader:\n        raw, mel, labels = raw.to(device), mel.to(device), labels.to(device)\n        optimizer.zero_grad()\n        output = model(raw, mel)\n        loss = loss_fn(output, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        correct += (output.argmax(1) == labels).sum().item()\n    acc = correct / len(train_loader.dataset)\n    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Train Acc: {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:42:21.506375Z","iopub.execute_input":"2025-09-29T04:42:21.507187Z","iopub.status.idle":"2025-09-29T04:51:56.969077Z","shell.execute_reply.started":"2025-09-29T04:42:21.507161Z","shell.execute_reply":"2025-09-29T04:51:56.968144Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 30.0601, Train Acc: 0.7632\nEpoch 2, Loss: 46.1449, Train Acc: 0.6053\nEpoch 3, Loss: 26.7090, Train Acc: 0.9211\nEpoch 4, Loss: 25.6626, Train Acc: 0.9211\nEpoch 5, Loss: 18.2720, Train Acc: 0.9211\nEpoch 6, Loss: 6.4229, Train Acc: 0.9211\nEpoch 7, Loss: 10.8289, Train Acc: 0.7895\nEpoch 8, Loss: 4.5353, Train Acc: 0.8421\nEpoch 9, Loss: 8.4065, Train Acc: 0.8947\nEpoch 10, Loss: 6.4213, Train Acc: 0.9211\nEpoch 11, Loss: 2.3391, Train Acc: 0.8684\nEpoch 12, Loss: 3.3792, Train Acc: 0.8158\nEpoch 13, Loss: 2.6957, Train Acc: 0.8947\nEpoch 14, Loss: 2.0673, Train Acc: 0.9737\nEpoch 15, Loss: 2.4337, Train Acc: 0.9211\nEpoch 16, Loss: 1.4717, Train Acc: 0.8684\nEpoch 17, Loss: 1.7305, Train Acc: 0.8684\nEpoch 18, Loss: 0.6635, Train Acc: 0.9474\nEpoch 19, Loss: 1.5314, Train Acc: 0.9211\nEpoch 20, Loss: 0.9681, Train Acc: 0.9211\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"torch.save(model.state_dict(), \"specRNet_rawGATST_fusion_adam_fulldataset_deep-voice-recog.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:53:49.935223Z","iopub.execute_input":"2025-09-29T04:53:49.935674Z","iopub.status.idle":"2025-09-29T04:53:50.026818Z","shell.execute_reply.started":"2025-09-29T04:53:49.935598Z","shell.execute_reply":"2025-09-29T04:53:50.025949Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"#save checkpoint model state \n\n#tukar nama model everytime tukar nama model or anything.. baca paper","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nmodel.eval()\n\nmisclassified = []\ncorrect_pred = []\n\nwith torch.no_grad():\n    for raw_batch, mel_batch, labels_batch in test_loader:\n        raw_batch, mel_batch, labels_batch = raw_batch.to(device), mel_batch.to(device), labels_batch.to(device)\n\n        outputs = model(raw_batch, mel_batch)\n        probs = F.softmax(outputs, dim=1)\n        preds = torch.argmax(probs, dim=1)\n\n        for i in range(len(labels_batch)):\n            true_label = labels_batch[i].item()\n            pred_label = preds[i].item()\n            confidence = probs[i][pred_label].item()\n\n            entry = {\n                \"true\": \"FAKE\" if true_label == 1 else \"REAL\",\n                \"pred\": \"FAKE\" if pred_label == 1 else \"REAL\",\n                \"confidence\": confidence\n            }\n\n            if true_label != pred_label:\n                misclassified.append(entry)\n            else:\n                correct_pred.append(entry)\n\nprint(f\"âœ… Total Correct: {len(correct_pred)}\")\nprint(f\"âŒ Total Misclassified: {len(misclassified)}\")\n\n# Show some misclassifications\nprint(\"\\nðŸ”Ž Sample Misclassifications:\")\nfor item in misclassified[:10]:  # show first 10\n    print(f\"True={item['true']} | Pred={item['pred']} | Conf={item['confidence']*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:53:59.302239Z","iopub.execute_input":"2025-09-29T04:53:59.302891Z","iopub.status.idle":"2025-09-29T04:54:21.336028Z","shell.execute_reply.started":"2025-09-29T04:53:59.302868Z","shell.execute_reply":"2025-09-29T04:54:21.335107Z"}},"outputs":[{"name":"stdout","text":"âœ… Total Correct: 11\nâŒ Total Misclassified: 2\n\nðŸ”Ž Sample Misclassifications:\nTrue=REAL | Pred=FAKE | Conf=98.37%\nTrue=REAL | Pred=FAKE | Conf=99.46%\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import time\nimport torch.nn.functional as F\n\n# Move model to device (if not already)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Get one batch from the validation loader\ndata_iter = iter(test_loader)\nraw_batch, mel_batch, labels_batch = next(data_iter)\n\n# Pick one sample (e.g. index 0)\nraw_sample = raw_batch[2].unsqueeze(0).to(device)  # move to GPU\nmel_sample = mel_batch[2].unsqueeze(0).to(device)  # move to GPU\nlabel = labels_batch[2].item()\n\n# Start timing\nstart_time = time.time()\n\n# Run inference\nmodel.eval()\nwith torch.no_grad():\n    output = model(raw_sample, mel_sample)\n    prediction = torch.argmax(output, dim=1).item()\n    probs = F.softmax(output, dim=1)\n    confidence = probs[0][prediction].item()\n\nend_time = time.time()\ninference_time = end_time - start_time\n\n# Print results\nprint(f\"True Label: {'FAKE' if label == 1 else 'REAL'}\")\nprint(f\"Prediction: {'FAKE' if prediction == 1 else 'REAL'} ({confidence*100:.2f}%)\")\nprint(f\"Inference Time: {inference_time:.4f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T04:59:33.081920Z","iopub.execute_input":"2025-09-29T04:59:33.082248Z","iopub.status.idle":"2025-09-29T04:59:44.029173Z","shell.execute_reply.started":"2025-09-29T04:59:33.082209Z","shell.execute_reply":"2025-09-29T04:59:44.028284Z"}},"outputs":[{"name":"stdout","text":"True Label: FAKE\nPrediction: FAKE (99.60%)\nInference Time: 0.0035 seconds\n","output_type":"stream"}],"execution_count":34}]}